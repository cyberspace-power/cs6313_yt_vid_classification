{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "thumbnail_playground.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v1mS2IMwANXz"
      },
      "outputs": [],
      "source": [
        "# CNN models for image classification\n",
        "# 1. ResNet\n",
        "# 2. InceptionNet/GoogLeNet (inception v4)\n",
        "# 3. MobileNet\n",
        "\n",
        "# RNN models for text classification\n",
        "# 1. Logistic Regression\n",
        "# 2. RNN \n",
        "# 3. LSTM\n",
        "# 4. BERT (optional)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !git clone https://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@github.com/elixir-1/cs6313_yt_vid_classification.git\n",
        "!git clone https://github.com/elixir-1/Introduction-to-Computational-Thinking.git\n",
        "\n",
        "# git clone https://github.com/cyberspace-power/cs6313_yt_vid_classification.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RmAHSkuPlaU9",
        "outputId": "3da1e18e-f892-4b6a-9908-a158319ff6c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Introduction-to-Computational-Thinking'...\n",
            "remote: Enumerating objects: 74, done.\u001b[K\n",
            "remote: Counting objects: 100% (74/74), done.\u001b[K\n",
            "remote: Compressing objects: 100% (52/52), done.\u001b[K\n",
            "remote: Total 74 (delta 29), reused 60 (delta 18), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (74/74), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Conv2D\n",
        "from tensorflow.keras.layers import MaxPool2D, Flatten, Dense, Dropout\n",
        "from keras.layers import Concatenate\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.models import Sequential\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "from tensorflow.keras.applications import ResNet50V2"
      ],
      "metadata": {
        "id": "V0tV4FxbVytD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !cd drive/\n",
        "# !ls drive/MyDrive/\n",
        "# image_file_path = \"drive/MyDrive/category_wise_thumbnails\"\n",
        "# image = cv2.imread(image_file_path+\"/1/_2e_1JRV2HY.jpg\")\n",
        "# plt.imshow(image)\n",
        "# print(image.shape)"
      ],
      "metadata": {
        "id": "9382MJEWbQLD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# text_file_path = \"drive/MyDrive/category_wise_data\"\n",
        "# data = pd.read_csv(text_file_path+\"/category_1.csv\")\n",
        "# print(data.head())\n",
        "# print(data[0].title)\n",
        "# print(data[0].description)"
      ],
      "metadata": {
        "id": "JrFereIZ6xhn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# IMG_SIZE_R = 480\n",
        "# IMG_SIZE_C = 360"
      ],
      "metadata": {
        "id": "usXuFNZEaCuG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from tqdm import tqdm\n",
        "# image_filepath = '/content/drive/MyDrive/category_wise_thumbnails/'\n",
        "\n",
        "# def read_image_data(categories = ['1']):\n",
        "\n",
        "#     image_data = []\n",
        "#     label_data = []\n",
        "#     title_data = []\n",
        "\n",
        "#     # with open('/content/drive/MyDrive/category_wise_thumbnails/17') as filepath:\n",
        "#     for x in categories:\n",
        "#       for img in tqdm(os.listdir(image_filepath+x), desc = \"loading\"):\n",
        "\n",
        "#         image_path = os.path.join(image_filepath+x+'/', img)\n",
        "\n",
        "#         # loading the image from the path and then converting them into\n",
        "#         # grayscale for easier covnet prob\n",
        "#         thumbnail = cv2.imread(image_path)\n",
        "#         thumbnail=cv2.resize(thumbnail, (224, 224, 3),interpolation = cv2.INTER_AREA)\n",
        "#         thumbnail=np.array(thumbnail)\n",
        "#         thumbnail = thumbnail.astype('float32')\n",
        "#         thumbnail /= 255\n",
        "#         # thumbnail = cv2.resize(thumbnail, (IMG_SIZE_R, IMG_SIZE_C))\n",
        "\n",
        "#         # resizing the image for processing them in the covnet\n",
        "#         # img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
        "\n",
        "#         # final step-forming the training data list with numpy array of the images\n",
        "#         image_data.append(np.array(thumbnail))\n",
        "#         label_data.append(int(x))\n",
        "#         title_data.append(img)\n",
        "    \n",
        "#     df = pd.DataFrame({'Image':image_data, 'Category':label_data, 'Video Id':title_data})\n",
        " \n",
        "#     # shuffling of the training data to preserve the random state of our data\n",
        "#     # from random import shuffle\n",
        "#     # shuffle(training_data)\n",
        "\n",
        "#     df = df.sample(frac=1).reset_index(drop=True)\n",
        "#     # print(df.head())\n",
        "#     # print(df.shape)\n",
        " \n",
        "#     # saving our trained data for further uses if required \n",
        "#     # np.save('train_data.npy', training_data)\n",
        "#     return df"
      ],
      "metadata": {
        "id": "hOdMp155V5lv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# text_filepath = '/content/drive/MyDrive/category_wise_data/category_'\n",
        "# def read_text_data(categories = ['1']):\n",
        "#   dfs = [pd.read_csv(text_filepath+x+'.csv', index_col = 0) for x in categories]\n",
        "#   df = pd.concat(dfs)\n",
        "#   df.Category = df.Category.astype(int)\n",
        "#   print(df.head())\n",
        "#   print(df.shape)\n",
        "#   return df"
      ],
      "metadata": {
        "id": "yUplXRYaWqsP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# titles = read_text_data()\n",
        "# images = read_image_data()\n",
        "\n",
        "# data = pd.merge(titles, images, on='Video Id',  how='outer')\n",
        "# print(data.head())\n",
        "# print(data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ciLRspfFZvwK",
        "outputId": "fb90af10-5afb-4499-b925-e914bee4c582"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Video Id                                              Title  \\\n",
            "0  qu7p_7185BM                 Another Chance - Joseph & Faye SDE   \n",
            "1  6M7c1785Nh8                                Black Sheep Trailer   \n",
            "2  JLoQ65Hr2YA                         Slow Motion Fingerboarding   \n",
            "3  HReH29s_NbY  superman vs batman / paint animation / windows...   \n",
            "4  RbhOaG64IVM                                D.Gray-Man cool amv   \n",
            "\n",
            "                                         Description  Category  \\\n",
            "0  video by Nino Ventura Films (www.ninoventurafi...         1   \n",
            "1  Terrified of sheep and dosed up on therapy, He...         1   \n",
            "2  Yellow Board - Matthew Knoblauch\\r\\nBlue Board...         1   \n",
            "3  heres  another paint animation ,this time its ...         1   \n",
            "4           a cool d.gray-man amv i made its awesome         1   \n",
            "\n",
            "                                              URL  \n",
            "0  https://i.ytimg.com/vi/qu7p_7185BM/default.jpg  \n",
            "1  https://i.ytimg.com/vi/6M7c1785Nh8/default.jpg  \n",
            "2  https://i.ytimg.com/vi/JLoQ65Hr2YA/default.jpg  \n",
            "3  https://i.ytimg.com/vi/HReH29s_NbY/default.jpg  \n",
            "4  https://i.ytimg.com/vi/RbhOaG64IVM/default.jpg  \n",
            "(4724, 5)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading: 100%|██████████| 4460/4460 [00:28<00:00, 155.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Video Id                                              Title  \\\n",
            "0  qu7p_7185BM                 Another Chance - Joseph & Faye SDE   \n",
            "1  6M7c1785Nh8                                Black Sheep Trailer   \n",
            "2  JLoQ65Hr2YA                         Slow Motion Fingerboarding   \n",
            "3  HReH29s_NbY  superman vs batman / paint animation / windows...   \n",
            "4  RbhOaG64IVM                                D.Gray-Man cool amv   \n",
            "\n",
            "                                         Description  Category_x  \\\n",
            "0  video by Nino Ventura Films (www.ninoventurafi...         1.0   \n",
            "1  Terrified of sheep and dosed up on therapy, He...         1.0   \n",
            "2  Yellow Board - Matthew Knoblauch\\r\\nBlue Board...         1.0   \n",
            "3  heres  another paint animation ,this time its ...         1.0   \n",
            "4           a cool d.gray-man amv i made its awesome         1.0   \n",
            "\n",
            "                                              URL Image  Category_y  \n",
            "0  https://i.ytimg.com/vi/qu7p_7185BM/default.jpg   NaN         NaN  \n",
            "1  https://i.ytimg.com/vi/6M7c1785Nh8/default.jpg   NaN         NaN  \n",
            "2  https://i.ytimg.com/vi/JLoQ65Hr2YA/default.jpg   NaN         NaN  \n",
            "3  https://i.ytimg.com/vi/HReH29s_NbY/default.jpg   NaN         NaN  \n",
            "4  https://i.ytimg.com/vi/RbhOaG64IVM/default.jpg   NaN         NaN  \n",
            "(9184, 7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# data.to_pickle('dataset_1_17_24_25.pickle')"
      ],
      "metadata": {
        "id": "K6JBfWhXJ9I0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# image_df = pd.read_pickle('/content/drive/MyDrive/images_1_2.pickle')"
      ],
      "metadata": {
        "id": "GB4IIN0E4F4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# X_train, y_train, X_valid, y_valid, X_test, y_test = train_valid_test_split(data, target = 'Category', \n",
        "#                                                                             train_size=0.8, valid_size=0.1, test_size=0.1)\n",
        "# print(X_train.shape)\n",
        "# print(X_valid.shape)\n",
        "# print(X_test.shape)\n",
        "\n",
        "# print(y_train.shape)\n",
        "# print(y_valid.shape)\n",
        "# print(y_test.shape)"
      ],
      "metadata": {
        "id": "n8vulmUjgNGe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "image_filepath = 'cs6313_yt_vid_classification/dataset/category_wise_thumbnails/'\n",
        "\n",
        "def read_image_data(categories = ['1', '2', '10', '17']):\n",
        "\n",
        "    image_data = []\n",
        "    label_data = []\n",
        "    title_data = []\n",
        "\n",
        "    # with open('/content/drive/MyDrive/category_wise_thumbnails/17') as filepath:\n",
        "    for x in categories:\n",
        "      for img in tqdm(os.listdir(image_filepath+'category_'+x), desc = \"loading\"):\n",
        "\n",
        "        image_path = os.path.join(image_filepath+'category_'+x+'/', img)\n",
        "\n",
        "        # loading the image from the path and then converting them into\n",
        "        # grayscale for easier covnet prob\n",
        "        thumbnail = cv2.imread(image_path)\n",
        "        # thumbnail=cv2.resize(thumbnail, (224, 224, 3),interpolation = cv2.INTER_AREA)\n",
        "        thumbnail = np.array(thumbnail)\n",
        "        thumbnail = thumbnail.astype('float32')\n",
        "        thumbnail /= 255\n",
        "\n",
        "        # final step-forming the training data list with numpy array of the images\n",
        "        image_data.append(np.array(thumbnail))\n",
        "        label_data.append(int(x))\n",
        "        title_data.append(img)\n",
        "    \n",
        "    df = pd.DataFrame({'Image':image_data, 'Category':label_data, 'Video Id':title_data})\n",
        " \n",
        "    # shuffling of the training data to preserve the random state of our data\n",
        "    # from random import shuffle\n",
        "    # shuffle(training_data)\n",
        "\n",
        "    df = df.sample(frac=1).reset_index(drop=True)\n",
        "    # print(df.head())\n",
        "    # print(df.shape)\n",
        " \n",
        "    # saving our trained data for further uses if required \n",
        "    # np.save('train_data.npy', training_data)\n",
        "    return df"
      ],
      "metadata": {
        "id": "JBD-2AvlpFKO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_df = read_image_data()\n",
        "# image_df = pd.read_csv('images.csv')    #CHAGE TO GIT CLONE\n",
        "# image_df = image_df.sample(frac=1).reset_index(drop=True)\n",
        "print(image_df.shape)\n",
        "print(image_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8k47otXqMkRO",
        "outputId": "bbbe0f4f-6cc3-43be-fd07-559a2656223f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(7594, 3)\n",
            "                                               Image  Category  \\\n",
            "0  [[[128 145 142]\\n  [128 145 142]\\n  [129 146 1...         1   \n",
            "1  [[[0 0 0]\\n  [0 0 0]\\n  [0 0 0]\\n  ...\\n  [0 0...         1   \n",
            "2  [[[0 0 0]\\n  [0 0 0]\\n  [0 0 0]\\n  ...\\n  [0 0...         2   \n",
            "3  [[[234 194 142]\\n  [236 196 144]\\n  [240 200 1...         1   \n",
            "4  [[[0 0 0]\\n  [0 0 0]\\n  [0 0 0]\\n  ...\\n  [0 0...         1   \n",
            "\n",
            "          Video Id  \n",
            "0  2s83o0vFF50.jpg  \n",
            "1  MmZ1HWPMh9o.jpg  \n",
            "2  nCgVyC7g5H8.jpg  \n",
            "3  rHzx8NXk0w8.jpg  \n",
            "4  J2tMLvIfKuA.jpg  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CNN model"
      ],
      "metadata": {
        "id": "KHOj--ma0OGy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import preprocessing\n",
        "le = preprocessing.LabelEncoder()\n",
        "le = le.fit(image_df['Category'])\n",
        "le.classes_\n",
        "category = le.transform(image_df['Category'])\n",
        "image_df['Category'] = category\n",
        "image_df = image_df[['Image', 'Category']]\n",
        "print(image_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pABN7E6TNcKg",
        "outputId": "d336cab1-05c8-4e5a-fb9e-26f22514a972"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                               Image  Category\n",
            "0  [[[128 145 142]\\n  [128 145 142]\\n  [129 146 1...         0\n",
            "1  [[[0 0 0]\\n  [0 0 0]\\n  [0 0 0]\\n  ...\\n  [0 0...         0\n",
            "2  [[[0 0 0]\\n  [0 0 0]\\n  [0 0 0]\\n  ...\\n  [0 0...         1\n",
            "3  [[[234 194 142]\\n  [236 196 144]\\n  [240 200 1...         0\n",
            "4  [[[0 0 0]\\n  [0 0 0]\\n  [0 0 0]\\n  ...\\n  [0 0...         0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#TODO FIX newlines here, image converted to string "
      ],
      "metadata": {
        "id": "OxYfkrJXYr8K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(image_df['Image'][0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T3puo27-WI5I",
        "outputId": "7cdfbaa4-b828-4b83-b390-0146131f26c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'str'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train, test = train_test_split(image_df, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "Ji3PhRdhNJ8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = train['Image']\n",
        "y_train = train['Category']\n",
        "\n",
        "X_test = test['Image']\n",
        "y_test = test['Category']\n",
        "\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kwMAhCb7NTPO",
        "outputId": "3b84c037-92c4-4584-b09c-aa37b82a72c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(6075,)\n",
            "(6075,)\n",
            "(1519,)\n",
            "(1519,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "y4-K9hw4YGNi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CNN base model\n",
        "\n",
        "\n",
        "resnet_base = tf.keras.applications.ResNet50(\n",
        "    include_top=False,\n",
        "    weights=\"imagenet\",\n",
        "    input_tensor=None,\n",
        "    input_shape=(224, 224, 3),\n",
        "    pooling='avg',\n",
        "    classes=2,\n",
        "    **kwargs\n",
        ")\n",
        "\n",
        "# resnetv2_base = tf.keras.applications.ResNet50V2(\n",
        "#     include_top=False,\n",
        "#     weights=\"imagenet\",\n",
        "#     pooling='avg'\n",
        "# )\n",
        "\n",
        "# Add a classifier\n",
        "x = Flatten()(resnet_base.output)\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "x = Dropout(0.2)(x)\n",
        "x = Dense(4, activation='softmax')(x) # change 2 to total number of classes\n",
        "cnn_model = tf.keras.models.Model(resnet_base.input, x)\n",
        "cnn_model.compile(optimizer=Adam(learning_rate=0.00001),\n",
        "    loss='categorical_crossentropy',  # change to categorical with shape[0,0,...1,0,0..] with length as the total number of thumbnail categories\n",
        "    metrics=['accuracy'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "D4yR1mKO0KNZ",
        "outputId": "f34b4f4a-b58a-4740-86dc-12161add2e3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-656af8cc4785>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# CNN base model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m resnet_base = tf.keras.applications.ResNet50(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0minclude_top\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"imagenet\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0minput_tensor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = ModelCheckpoint(\"resnet_model.h5\", monitor='val_accuracy', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
        "early = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=20, verbose=1, mode='auto')\n",
        "hist = cnn_model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=2, batch_size=32, callbacks=[checkpoint, early])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 802
        },
        "id": "EYP84xmX1aMo",
        "outputId": "becb58cc-ee43-47d7-efdf-f3258af82621"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/2\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 480, 360, 3) for input KerasTensor(type_spec=TensorSpec(shape=(None, 480, 360, 3), dtype=tf.float32, name='input_2'), name='input_2', description=\"created by layer 'input_2'\"), but it was called on an input with incompatible shape (None, 1).\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-03bfcaf19367>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"resnet_model.h5\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_accuracy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_weights_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mearly\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_accuracy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_delta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mhist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnn_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1146\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1147\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1148\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 859, in train_step\n        y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/input_spec.py\", line 214, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" '\n\n    ValueError: Exception encountered when calling layer \"model_1\" (type Functional).\n    \n    Input 0 of layer \"conv1_pad\" is incompatible with the layer: expected ndim=4, found ndim=2. Full shape received: (None, 1)\n    \n    Call arguments received:\n      • inputs=tf.Tensor(shape=(None, 1), dtype=string)\n      • training=True\n      • mask=None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RNN LSTM model"
      ],
      "metadata": {
        "id": "M4uYdXkyRhM6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# max_features = 20000  # Only consider the top 20k words\n",
        "# maxlen = 200  # Only consider the first 200 words of each movie review\n",
        "\n",
        "# # Input for variable-length sequences of integers\n",
        "# inputs = keras.Input(shape=(None,), dtype=\"int32\")\n",
        "# # Embed each integer in a 128-dimensional vector\n",
        "# x = layers.Embedding(max_features, 128)(inputs)\n",
        "# # Add 2 bidirectional LSTMs\n",
        "# x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x)\n",
        "# x = layers.Bidirectional(layers.LSTM(64))(x)\n",
        "# x = layers.Flatten()(x)\n",
        "# outputs = layers.Dense(2, activation=\"softmax\")(x)\n",
        "# lstm_model = keras.Model(inputs, outputs)\n",
        "# lstm_model.compile(optimizer=Adam(learning_rate=0.00001),\n",
        "#     loss='categorical_crossentropy',  # change to categorical with shape[0,0,...1,0,0..] with length as the total number of thumbnail categories\n",
        "#     metrics=['accuracy'])\n",
        "\n",
        "# # # CNN base model\n",
        "# # resnetv2_base = tf.keras.applications.ResNet50V2(\n",
        "# #     include_top=False,\n",
        "# #     weights=\"imagenet\",\n",
        "# #     input_shape=(IMG_SIZE_R, IMG_SIZE_C, 3),\n",
        "# #     pooling='avg'\n",
        "# # )\n",
        "# # resnetv2_base = Flatten()(resnetv2_base.output)\n",
        "\n",
        "# # # Add a classifier\n",
        "# # merge = Concatenate([x, resnetv2_base])\n",
        "# # l1 = Dense(1024, activation='relu')\n",
        "# # l2 = Dropout(0.2)\n",
        "# # l3 = Dense(5, activation='softmax') # change 5 to total number of classes\n",
        "\n",
        "# # multimodal_model = Sequential()\n",
        "# # multimodal_model.add(merge)\n",
        "# # multimodal_model.add(l1)\n",
        "# # multimodal_model.add(l2)\n",
        "# # multimodal_model.add(l3)\n",
        "\n",
        "# # # multimodal_model = tf.keras.models.Model(base_model.input, x)\n",
        "# # multimodal_model.compile(optimizer=Adam(learning_rate=0.00001),\n",
        "# #     loss='categorical_crossentropy',  # change to categorical with shape[0,0,...1,0,0..] with length as the total number of thumbnail categories\n",
        "# #     metrics=['accuracy']\n",
        "# # )\n",
        "\n",
        "# # outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "# # model = keras.Model(inputs, outputs)\n",
        "# # [tf.keras.Input(shape=(480,360,3)), tf.keras.Input(shape=(None,))]\n",
        "# # multimodal_model.build()\n",
        "# # multimodal_model.summary()\n"
      ],
      "metadata": {
        "id": "WFbHeRWcI_9Z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "outputId": "1b6a6fb9-5d36-478c-85f0-198c94d08130"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-60e4621ce58d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;31m# model = keras.Model(inputs, outputs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m# [tf.keras.Input(shape=(480,360,3)), tf.keras.Input(shape=(None,))]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0mmultimodal_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0mmultimodal_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/sequential.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    342\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0minput_shape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'You must provide an `input_shape` argument.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    345\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_graph_network_for_inferred_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: You must provide an `input_shape` argument."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# checkpoint = ModelCheckpoint(\"lstm_model.h5\", monitor='val_accuracy', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
        "# early = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=20, verbose=1, mode='auto')\n",
        "# hist = lstm_model.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=2, batch_size=32, callbacks=[checkpoint, early])"
      ],
      "metadata": {
        "id": "VowsRJl8VCY-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# x = Flatten()(base_model.output)\n",
        "# x = Dense(1024, activation='relu')(x)\n",
        "# x = Dropout(0.2)(x)\n",
        "# x = Dense(2, activation='softmax')(x) # change 2 to total number of classes\n",
        "\n",
        "# model = tf.keras.models.Model(base_model.input, x)\n",
        "# model.compile(optimizer=Adam(learning_rate=0.00001),\n",
        "#     loss='binary_crossentropy',  # change to categorical with shape[0,0,...1,0,0..] with length as the total number of thumbnail categories\n",
        "#     metrics=['accuracy']\n",
        "# )\n",
        "# model.summary()"
      ],
      "metadata": {
        "id": "zfJrmJyAjraB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import random\n",
        "# labels = []\n",
        "# for i in range (10):\n",
        "#   first = random.choice([0,1])\n",
        "#   second = abs(first-1)\n",
        "#   labels.append([first, second])\n",
        "\n",
        "# labels\n",
        "\n",
        "\n",
        "# # experiment code"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "743j4rJIghQK",
        "outputId": "c5fefa2c-9324-4b67-ca2e-72b20a163957"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 0],\n",
              " [0, 1],\n",
              " [1, 0],\n",
              " [1, 0],\n",
              " [0, 1],\n",
              " [0, 1],\n",
              " [1, 0],\n",
              " [0, 1],\n",
              " [0, 1],\n",
              " [1, 0]]"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# checkpoint = ModelCheckpoint(\"resnet50.h5\", monitor='val_accuracy', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
        "# early = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=20, verbose=1, mode='auto')\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# X_train, X_test, y_train, y_test = train_test_split(np.array(images),np.array(labels), test_size=0.33)\n",
        "# hist = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=2, batch_size=32, callbacks=[checkpoint, early])"
      ],
      "metadata": {
        "id": "vFhntPJLj2xN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "941ed531-b418-4acd-b04c-d35d26281dcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/2\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.5040 - accuracy: 0.8333\n",
            "Epoch 1: val_accuracy improved from -inf to 0.00000, saving model to resnet50.h5\n",
            "1/1 [==============================] - 27s 27s/step - loss: 2.5040 - accuracy: 0.8333 - val_loss: 14.3296 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/2\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.4399 - accuracy: 0.8333\n",
            "Epoch 2: val_accuracy did not improve from 0.00000\n",
            "1/1 [==============================] - 8s 8s/step - loss: 2.4399 - accuracy: 0.8333 - val_loss: 15.9092 - val_accuracy: 0.0000e+00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# plt.plot(hist.history['accuracy'])\n",
        "# plt.plot(hist.history['val_accuracy'])\n",
        "# plt.plot(hist.history[\"loss\"])\n",
        "# plt.plot(hist.history['val_loss'])\n",
        "# plt.title(\"ResNet 50 Model Accuracy\")\n",
        "# plt.ylabel(\"Accuracy\")\n",
        "# plt.xlabel(\"Epoch\")\n",
        "# plt.legend([\"Accuracy\",\"Validation Accuracy\",\"Loss\",\"Validation Loss\"])\n",
        "# plt.show()\n",
        "# plt.savefig('resnet50_acc.png')"
      ],
      "metadata": {
        "id": "Anv7aI0dkMg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "saved_model = load_model(\"resnet50.h5\")\n",
        "test_x = []\n",
        "original_y = []\n",
        "parsed_dataset = read_dataset('/content/train0133_processed.tfrecord') # ACTION NEEDED: .tfrecord file_path needs to be specified\n",
        "labeled_data_sets = labeled_dataset(parsed_dataset)\n",
        "for data in labeled_data_sets[:800]:\n",
        "  if data[1] == 12:\n",
        "    val = 0\n",
        "  elif data[1] == 11:\n",
        "    val = 1\n",
        "  elif data[1] == 17:\n",
        "    val = 2\n",
        "  else:\n",
        "    val = 3\n",
        "  resized_image = cv2.resize(data[0], (224, 224)) / 255.0\n",
        "  test_x.append(resized_image)\n",
        "  original_y.append(val)\n",
        "test_y = np.array(original_y).T\n",
        "test_x = np.array(test_x)\n",
        "test_y = to_categorical(test_y)\n",
        "loss, acc = saved_model.evaluate(test_x, test_y)\n",
        "print(loss)\n",
        "print(acc)"
      ],
      "metadata": {
        "id": "6kYZsVNYkQZs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_y = saved_model.predict(test_x)\n",
        "predict_y = np.argmax(predict_y, axis=1)\n",
        "original_y = np.array(original_y)"
      ],
      "metadata": {
        "id": "k7v2DojOkSny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
        "precision = precision_score(original_y, predict_y, average='macro')\n",
        "recall = recall_score(original_y, predict_y, average='macro')\n",
        "f1 = f1_score(original_y, predict_y, average='macro')\n",
        "cm = confusion_matrix(original_y, predict_y)\n",
        "print(f'Precision score is: {precision}')\n",
        "print(f'Recall score is: {recall}')\n",
        "print(f'F1 score is: {f1}')\n",
        "print(cm)"
      ],
      "metadata": {
        "id": "WM3XBE8pkUpA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_confusion_matrix(cm,\n",
        "                          target_names,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=None,\n",
        "                          normalize=True):\n",
        "    \"\"\"\n",
        "    given a sklearn confusion matrix (cm), make a nice plot\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    cm:           confusion matrix from sklearn.metrics.confusion_matrix\n",
        "\n",
        "    target_names: given classification classes such as [0, 1, 2]\n",
        "                  the class names, for example: ['high', 'medium', 'low']\n",
        "\n",
        "    title:        the text to display at the top of the matrix\n",
        "\n",
        "    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n",
        "                  see http://matplotlib.org/examples/color/colormaps_reference.html\n",
        "                  plt.get_cmap('jet') or plt.cm.Blues\n",
        "\n",
        "    normalize:    If False, plot the raw numbers\n",
        "                  If True, plot the proportions\n",
        "\n",
        "    Usage\n",
        "    -----\n",
        "    plot_confusion_matrix(cm           = cm,                  # confusion matrix created by\n",
        "                                                              # sklearn.metrics.confusion_matrix\n",
        "                          normalize    = True,                # show proportions\n",
        "                          target_names = y_labels_vals,       # list of names of the classes\n",
        "                          title        = best_estimator_name) # title of graph\n",
        "\n",
        "    Citiation\n",
        "    ---------\n",
        "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
        "\n",
        "    \"\"\"\n",
        "    import matplotlib.pyplot as plt\n",
        "    import numpy as np\n",
        "    import itertools\n",
        "\n",
        "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
        "    misclass = 1 - accuracy\n",
        "\n",
        "    if cmap is None:\n",
        "        cmap = plt.get_cmap('Blues')\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "\n",
        "    if target_names is not None:\n",
        "        tick_marks = np.arange(len(target_names))\n",
        "        plt.xticks(tick_marks, target_names, rotation=45)\n",
        "        plt.yticks(tick_marks, target_names)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "\n",
        "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        if normalize:\n",
        "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
        "                     horizontalalignment=\"center\",\n",
        "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "        else:\n",
        "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
        "                     horizontalalignment=\"center\",\n",
        "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "knrOFmWJktg7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_confusion_matrix(cm, ['Arts', 'Games', 'Vehicles', 'Other'], title='ResNet50 Confusion Matrix')"
      ],
      "metadata": {
        "id": "2EezjwCOkwCd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}